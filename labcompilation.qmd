---
project:
  type: website

website:
  title: "CIS9660 Lab Compilation"
  author: "Chris Panlasigui, Yuanfeng Cai"
format: 
  html:
    code-tools: true
    code-block-border-left: "#31BAE9"
    toc: true
    toc-depth: 3
    toc-title: Contents
    code-fold: show
    code-summary: "Code"
    code-overflow: wrap 
editor: visual
execute:
  output: false
---


# Exploratory Data Analysis {#sec-1eda}

## Loading Data {#sec-loaddata}

Before the lab, please download and save data `AutoLab1.csv` from Blackboard-Data.

```{r}
# use file.choose() if you want a window to open for you to locate your file
# Auto=read.csv(file.choose(),header=T, stringsAsFactors=TRUE)

# if you know the path to your file you can directly include it and omit file.choose()
Auto=read.csv(file="../raw_data/autolab1.csv",
              header=T, 
              stringsAsFactors=TRUE)
```

You can use function `read.table()` or `read.csv()` to import data into R, depending on the file type. You can use `help(read.csv)` or `?read.csv` to find more about how to use this function. Since the file type is .csv, we use `read.csv()`in this case. Use the following command to load `AutoLab1.csv` into R and store it as an object called Auto.

The command, `file.choose()` locates the file from your local drive, the option `header=T` (or `header=TRUE`) means that the first line of the file contains the variable names, the option `stringsAsFactors=TRUE` (or `stringsAsFactors=T`) means that converting characters to factors

## Data Structure {#sec-1datastruct}

```{r}
head(Auto)
dim(Auto)
str(Auto)
```

You can use `head()` to look at the first 6 rows. Function `dim()` can output the number of rows followed by the number of columns and function `str()` can return the data structure.

## Summary Statistics {#sec-1summarystat}

```{r}
summary(Auto)
```

We can use function `summary()` to output the summary statistics of data.

## Data Restructuring {#sec-1datarestruct}

```{r}
Auto[,2]=as.factor(Auto[,2])
summary(Auto[,2])
```

The variable cylinders (i.e. the second column) is stored as a quantitative variable. As it only has a small number of possible values, we can convert it to a qualitative variable. Function as.factor() converts a quantitative variable into a qualitative variable. Check the summary statistics of cylinders to see if it is the same as the output in @sec-1summarystat.

## Graphs {#sec-1edagraphs}

### Scatterplot {#sec-1edascatter}

```{r}
plot(x=Auto$horsepower, y=Auto$mpg)
```

We can use function plot() to produce plots. However, simply typing the variable names does not work, because R does not know where to find those variables. You can either use the `$`sign or 

```{r}
attach(Auto)
names(Auto)
plot(x=horsepower , 
     y=mpg, col ="red", 
     xlab="Horsepower",
     ylab ="MPG ",
     xlim=c(30,250), 
     ylim=c(5,50), 
     main="Horsepower vs. MPG")
```

Use function `attach()` to tell R to make the variables available by name. Function `names()` lists all variable names. Then you can use the variable name directly.

Here the option `col ="red"` tells R that color data points red. The options `xlab="Horsepower",ylab ="MPG"`, and `main="Horsepower vs. MPG"` tell R the x axis title, the y axis title and the main title respectively.

The options `xlim` and `ylim` tell R the range of x axis and y axis.
There are many other optional parameters in function `plot()`, which we do not include in this case. You can use `help(plot)` or `?plot` to explore more about them.

```{r}
par(mfrow=c(1,2))
plot(x=acceleration ,
     y=mpg, 
     col ="red", 
     xlab="Acceleration",
     ylab ="MPG ", 
     main="Acceleration vs. MPG")

plot(x=weight , 
     y=mpg, 
     col ="red", 
     xlab="Weight", 
     ylab ="MPG ", 
     main="Weight vs. MPG")
```

We can use function `par(mfrow=c(nrows,ncols))` to combine multiple plots into one graph. For example, `par(mfrow=c(3,1))` indicates that three figures will be arranged in 3 rows and 1 column. Now letâ€™s generate another two figures and arrange them in one column.

```{r}
pairs(Auto)
pairs(Auto[c(3:5)])
```

The `pairs()` function creates a scatterplot for every scatterplot pair of variables. We can also produce scatterplots matrix for just a subset of the variables.


### Barplot {#sec-edabarplot}

```{r}
par(mfrow=c(1,1))
plot(x=cylinders, 
    y=mpg, 
    col="red", 
    varwidth=TRUE, 
    xlab=" Cylinders ", 
    ylab="MPG ", 
    main="Cylinders vs. MPG")
```

If the variable plotted on the x-axis is categorical, then boxplots will automatically be produced.

### Histogram {#sec-edahist}

```{r}
hist(x=mpg, 
     breaks=10, 
     col="red", 
     xlab="MPG ",
     xlim=c(0,50),
     main="Histogram of MPG")
```

We can use `hist()` function to plot a histogram. The option `â€œbreaks=10â€` sets the total number of bins.


# Linear Regression {#sec-2linearregression}

## Load Data {#sec-2loaddata}

Before the lab, please download Data Lab2Data.csv from Blackboardâ†’Data. Then load Data Lab2Data.csv to object Datlab2.

```{r}
# delete global environment
rm(list = ls())

# load data
Datlab2 = read.csv(file="../raw_data/Lab2Data.csv", 
                   header=T, 
                   stringsAsFactors=TRUE)

# check column/variable names
names(Datlab2)

attach(Datlab2)
```

There are four input variables $(ð‘¥_1 ...ð‘¥_4)$ and one response variable $y$ in this data. Variable $x_4$ is a categorical variable with two possible values. We should create a dummy variable that takes on two possible numerical values. 

```{r}
contrasts(x4)
```

The `contrasts()` function returns how R codes this dummy variable.

## Build Model {#sec-2buildmodel}

### Full Model {#sec-2fullmodel}

```{r}
Model1=lm(y~x1+x2+x3+x4,data= Datlab2)
```

We first fit the entire data with all input variables into a multiple linear regression model Model1. We can use the `lm()` function:

$$ y =\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon$$ {#eq-linearm1}

```{r}
summary(Model1)
```

We can use the function `summary()` to obtain the coefficients, their associated `p-values`, and the $R^2$. We can find that only one variable looks insignificant in this model.

### Model with interaction effect {#sec-2interactioneffect}

```{r}
Model2=lm(y~x3*x4)
summary(Model2)
```


We can also check if the model should include an interaction effect. For example, to test an interaction effect between $x_3$ and $x_4$, we can include `lm(y~x3*x4)`. The syntax `x3*x4` in the function `lm()` simultaneously includes $x_3$, $x_4$, and $x_3 \times x_4$ð‘¥4. That is, `lm(y~x3*x4)` fits the following model:

$$y = \beta_0 + \beta_1 x_3 + \beta_2 x_4 + \beta_3(x_3 \times x_4) + \epsilon$$ {#eq-linearm2}
The result suggests that the interaction effect is significant.

```{r}
plot(x=predict(Model2), y=residuals(Model2))
plot(x=predict(Model2), y=rstudent(Model2))
```

We can generate diagnostic plots for models. The function `rstudent()` returns the studentized residuals, which can be used to identity outliers. Given the diagnostics results, there is no outlier in `Model2`.


## Resampling Methods {#sec-2resampling}

Now we try two candidate models `M1` and `M2`.  We will use re-sampling methods to compare their performance. 

$$M1: y =\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5(x_3 \times x_4) +  \epsilon$$ {#eq-M1}

$$M2: y = \beta_0 + \beta_1 x_3 + \beta_2 x_4 + \beta_3(x_3 \times x_4) + \epsilon$$ {#eq-M2}


### Hold-out {#sec-2holdout}

We use two types of re-sampling methods. First, we illustrate how to compare the models using hold-out. We use 80-20 hold-out in this case.  We develop models using a training data set and assess the model performance using a test data set. 

```{r}
# reproduceable random sampling
set.seed(1)

#index for training data set
train=sample(nrow(Datlab2),nrow(Datlab2)*0.8) 

#training data set
Datlab2.train=Datlab2[train, ] 

#test data set
Datlab2.test=Datlab2[-train, ] 

#the response vector in the test set
y.test=y[-train] 
```

We first use the `sample()` function to randomly split the original data set into one training set and one test set.

The function `nrow()` counts the total number of rows in `Datlab2`. Sometimes we want to reproduce the exact same sampling results; we can use the `set.seed()` function. To use this function, you need to set a seed, which is an arbitrary integer, e.g. 1.

```{r}
# full model with train data
M1train=lm(y~x1+x2+x3*x4, data=Datlab2.train)

# interaction model with train data
M2train =lm(y~x1+x3*x4, data=Datlab2.train)
```

The first model of choice `M1` contains all four variables and one interaction term. The second model `M2` contains three input variables and one interaction term. We learn the two models only using the training data set.

```{r}
# test performance of full model
y.predictM1=predict(M1train,Datlab2.test)
# performance measure using MSE of full model
M1MSE=mean((y.test-y.predictM1)^2)

# test performance of interaction model
y.predictM2=predict(M2train,Datlab2.test)
# performance measure using MSE of interaction model
M2MSE=mean((y.test-y.predictM2)^2)
```

To compare the model performance, we calculate the MSE of each model using the testing data set. The function `predict()` predicts the value of the response variable for each observation in test data. `y.predictM1` stores the predicted value of each observation in the test data set using model `M1`, and `y.predictM2` stores the predicted value of each observation in the test data set using model `M2`. The `mean()` function here is to calculate the average prediction deviation, i.e. MSE, in the entire test data.

#### AIC, BIC, Adjusted $R^2$ 


```{r}
# other performance measure of full model
AIC(M1train) # AIC
BIC(M1train) # BIC
summary(M1train) #adj R2

# other performance measure of interaction model
AIC(M2train) # AIC
BIC(M2train) # BIC 
summary(M2train) # adj. R2
```

In addition to using MSE, the model can be assessed using functions `BIC()`, `AIC()` and adjusted R2 in `summary()`.

### Cross-Validation (CV)

Now we illustrate how to compare the models using cross-validation. We use 5-fold cross validation along with MSE in this case. 

```{r}
# reproduceable random sampling
set.seed(1) 

# number of folds
k=5

# empty container to hold MSE of full model using CV
M1CVMSE=rep(0,k)
# emtpy container to hold MSE of interaction model using CV
M2CVMSE=rep(0,k)
```

We first create a vector to store the accuracy results associated with each fold for each model. We set the initial values for this vector as zero.

```{r}
#| output: true
#| include: true

# split data into k=5 folds 
folds=sample(1:k,nrow(Datlab2),replace=TRUE)
folds
```

We use the `sample()` function to split the original data set into five folds. This creates a vector of random values between 1 through 5.  This assigns the indices of `Datlab2` to each fold.  

```{r}
for(j in 1:k) # iterate from 1 through k=5
{ 
  # model of the train data portion of cv 
  M1CV=lm(y~ x1+x2+x3*x4,data=Datlab2[folds!=j,]) 
  # obtain the performance (MSE) of the model M1CV by applying it to the test portion of cv  
  M1CVMSE[j]=mean((y-predict(M1CV,Datlab2))[folds==j]^2) }

for(j in 1:k)
{ M2CV=lm(y~ x1 +x3*x4,data=Datlab2[folds!=j,]) 
M2CVMSE[j]=mean((y-predict(M2CV,Datlab2))[folds==j]^2) }
```

During each iteration, the `Datlab2[folds!=j,]` selects indices that are not equal to the $jth$ iteration and assigns it as the train data and stores the model value in `M1CV`.  So during the first iteration, the data train data will be all the indices that are not assigned to 1 in the `folds` vector. 

`[folds==j]` selects the indices that are equal to the $jth$ iteration, which is the test data.  So, `(y-predict(M1CV,Datlab2))[folds==j]^2` is the standard error of the test data, which the difference between the actual values of `y` to the predicted values based on the model `M1CV`.  MSE is calculated by obtaining the mean of the standard error in each iteration and is stored in `M2CVMSE[j]` where `j` is the current iteration index. `M2CVMSE` will then have 5 values since `j` has been set from 1 through `k=5`.

```{r}
MeanM1MSE=mean(M1CVMSE) ###CVMSE-M1###
MeanM2MSE=mean(M2CVMSE) ###CVMSE-M2###
```

Finally, calculate the cross-validation MSE of `M1` (@eq-M1) and `M2` (@eq-M2) by obtaining the mean of each CV model. 

# Classification: Logistic Regression & KNN {#sec-3class1}

## Logistic Regression {#sec-3logistic}

```{r}
#| output: true
#| 
# clear global env (remove all stored obj) 
rm(list = ls())

# load data
Default=read.csv(file = "../raw_data/Default.csv",
                 header=T, stringsAsFactors=TRUE)
attach(Default)

# check the dimensions
dim(Default)
names(Default)
```

Load Data `Default.csv` to object Default. The original data set contains 10000 observations.

```{r}
glm.fit1=glm(default~balance+income+student,
             family="binomial",
             data=Default)
```

First, we can use the `glm()` function to fit model `glm.fit1` which includes all three independent variables: `student`, `balance` and `income`, with all the observations. 

```{r}
summary(glm.fit1)
coef(glm.fit1)
exp(coef(glm.fit1))
```

We can use the function `summary()` to obtain the coefficients, their associated p-values, model AIC and residual deviance. The function `exp()` calculates **odds ratio** of the coefficients.

### 80/20 Hold-out Logistic Regression Model

```{r}
set.seed(1)

# 80/20 hold-out re-sampling
# indices of 80% train data 
train=sample(nrow(Default),nrow(Default)*0.8)

# indices 20% test data
Default.test=Default[-train, ]

# y-values (default variable) of the 20% test data
test.truevalue=default[-train]
```

We next evaluate the prediction accuracy of this model. We use the `sample()` function to split the original data set into one training set and one test set. We randomly select 80% of the observations for training and
the remaining in the test set, `Default.test`. And save the true value of the testing set in the vector `test.truevalue`.


```{r}
glm.fit2=glm(default~balance+student+income,
             data=Default,
             subset=train,
             family=binomial)
```

We fit the data with all three independent variables into a logistic regression model `glm.fit2`. The *`subset=train`* option in `glm()` is to fit a regression using only the observations corresponding to the subset.


```{r}
glm.probs2=predict(glm.fit2,Default.test, type="response")
```

We evaluate the performance of the model using the test set (Default.test). We use the function `predict()` to calculate the *predicted probabilities* of the **default** in the test set and store them to `glm.pred2`. The `type="response"` option tells R to output probabilities not the logit.


```{r}
#| include: true
#| output: true

contrasts(Default$default)
```


R assigned `No=0` and `Yes=1` in the `default` variable.  This means the our classifiers for the model is defined as: 

$$\hat{C}(x) = \left\{
  \begin{matrix}
  1 \ \text{'above'} & \hat{p}(x) > 0.5 \\
  0 \ \text{'below'} & \hat{p}(x) \le 0.5
  \end{matrix}
  \right.
$$


```{r}
glm.pred2=rep("No",2000)
glm.pred2[glm.probs2>.5]="Yes"
```

We convert the predicted probabilities into a binary class label, `Yes` or `No`. The following commands create a vector of class predictions based on whether the predicted probability is greater than or less than `0.5`.

```{r}
# a more concise code is to use ifelse()
glm.pred2=ifelse(glm.probs2 > 0.5, 'Yes', 'No')
```

Based on the classifier, the predicted probabilities:

$$\hat{p}(x) = \hat{P}(Y=1 | X=x)$$


#### Prediction Accuracy of 80/20 Hold-out {#sec-3accuracy}

```{r}
table(glm.pred2,test.truevalue)
mean(glm.pred2==test.truevalue)
```

The `table()` function produce a *confusion matrix* to determine how many observations were correctly classified. We then use `mean()` function to calculate the *accuracy of the prediction*.


### Cross-Validation Logistic Regression Model

```{r}
#| output: true

# set number of folds
k=5

# randomly assign indices to folds
folds=sample(1:k,nrow(Default),replace=TRUE)

# first 10 and last 10 fold values
cat(head(folds, n=10),"..." ,tail(folds, n=10))
```

We next evaluate the 5-fold cross-validation prediction of the model.  Split data into k=5-folds.  The output shows the first 10 and last 10 fold assignment.  These are the first 10 and the last 10 rows (or index) of the data. 

```{r}
# zero vectors to hold accuracy values of cv method
accuracy=rep(0,k)

for(i in 1:k)
{
  # cv logistic reg model of train data
  glm.fit3=glm(default~balance+student+income, 
               family="binomial",
               data=Default[folds!=i,])
  
  # assign the current ith iteration as the test data
  Default.test=Default[folds==i, ]
  # obtain the probabilities using the test data
  glm.probs3=predict(glm.fit3,Default.test, 
                     type="response")

  glm.pred3=rep("No", nrow(Default[folds==i,]))
  glm.pred3[glm.probs3>.5]="yes"

  # y-value (default variable) of the test data in the ith iteration
  test.truevalue=default[folds==i]
  
  # calculate the accuracy
  accuracy[i]=mean(glm.pred3==test.truevalue)
}
```

We create a vector to store the accuracy result for each fold. We set the initial values for this vector as zero.  R treats logical `TRUE=1` and `FALSE=0`.  So the `mean(glm.pred3==test.truevalue)` tests if the actual value of `default` is predicted accurately in `glm.pred3` in each row.  If they match up, then the return value is TRUE=1, if they do not, then the return value is FALSE=0.  At each iteration, the prediction accuracy calculated by obtaining the average and is stored into `accuracy[i]`

### Prediction Accuracy of CV Logistic Regression

```{r}
mean(accuracy)
```

Then we calculate the accuracy of the CV Logistic model by obtaining the grand average accuracy.


## KNN

```{r}
library(class)
```

### Data Scaling

```{r}
standardized.balance=scale(balance)
standardized.income=scale(income)
```


We first normalize the two quantitative input variables balance and income so that they would be on a comparable scale. The function `scale()` **standardizes** the quantitative variables.


```{r}
Input.standard=cbind(standardized.balance,standardized.income,student)
accuracy=matrix(0,10,5)
```

Then we use the function `cbind()` to combine the two standardized variables and the qualitative input
variable together.

```{r}
set.seed(2)
folds=sample(1:5,nrow(Input.standard),replace=TRUE)

for (j in 1:10) # 10 neighbors considered
{
  for(i in 1:5) # for each fold
  {
    # train data indices 
    train.standard=Input.standard[folds!=i,]
    # test data indices
    test.standard=Input.standard[folds==i,]
    
    # y-values (default var) of train data
    train.truevalue=default[folds!=i]
    # y-values (default var) of test data 
    test.truevalue=default[folds==i]
    
    
    knn.pred=knn(train=train.standard,
                 test=test.standard,
                 cl=train.truevalue,
                 k=j)
    accuracy[j,i]=mean(knn.pred==test.truevalue)
  }
}
```

To use KNN, we need to determine K. We can use 5-fold cross-validation to select the best K from `[1,10]`. Thus, we first create a matrix to store the accuracy results for five folds and ten different K values. We set the initial values for this matrix as zero.

`train.standard` is the input matrix of the training set and test.standard is the input matrix of the testing set.  `train.truevalue` is the original value of default in the training set and `test.truevalue` is the true value of
default in the testing set. For each observation in the testing set, the knn function can calculate its distance with each observation in the training set based on `train.standard` and `test.standard`. Given `k=j`, it selects the `k` most nearest neighbors, and use their default values (based on `train.truevalue`) to predict the default values in the testing set. In the knn function, we specify the number of neighbors in the option `k=j`. The output of this loop is the prediction accuracy

### Accuracy of KNN Model

```{r}
cv.accuracy=apply(accuracy,1,mean)
```

Then we calculate the average cross-validation accuracy for each `K`.

# Classification: CART, Bagging & Random Forest {#sec-4class2}

## Load Data

```{r}
#| output: true

rm(list = ls())
library(tree)

iris<-read.csv(file="../raw_data/iris.csv",
               header=T, 
               stringsAsFactors=TRUE)
names(iris)
attach(iris)
summary(iris)
```


It contains 3 classes of a total 150 instances. Each class refers to a type of iris plant. 

## 80/20 Hold-out Classification Tree

```{r}
# reproduceable random sampling seed
set.seed(1)

# indices of the traning data (80%)
train=sample(nrow(iris),nrow(iris)*0.8)

# tree model of the training data
tree.model=tree(Species~.,iris,subset =train)

# indices of the test data (20%)
iris.test=iris[-train,]

# values of the target variable (Species variable)
Species.test=Species[-train]
```

We first create a training set, and fit the tree using the training set.

```{r}
cv.model=cv.tree(object=tree.model, 
                 K=10, # k-fold cross validation
                 FUN=prune.misclass)
cv.model
```

We use `cv.tree()` to perform 10-fold cross validation, `K=10`,  to find the best subtree or the optimal way to prune the tree.  Although the re-sampling method is 80/20 holdout, `cv.tree()` does cross validation internally. 

```{r}
# prune tree model
prune.model=prune.tree(tree=tree.model,
                       best=5) # number of terminal nodes
plot(prune.model)
text(prune.model,pretty=0)
```

We prune tree to the size with the lowest cross-validation error rate using the `prune.tree()` function and plot the tree. We use the argument `FUN=prune.misclass` since it is the classification tree. It indicates that we want the classification error rate to guide the cross-validation and pruning process, rather than the default - deviance.

### Performance by Confusion Matrix

```{r}
prunetree.pred=predict(prune.model, # pruned tree model
                       iris.test, # apply pruned tree model on test data
                       type="class") # classification

table(prunetree.pred,Species.test)
```

Then we use the pruned tree to make predictions in the testing set and calculate the confusion matrix.

## Regression Tree

### Load data

```{r}
rm(list = ls())

library(tree)

Boston=read.csv("../raw_data/Boston.csv",header=T)
head(Boston)
dim(Boston)
```

Load Data `Boston.csv` to object Boston load package `tree` again.

### Regression Tree Model

```{r}
set.seed(1)

# 50/50 holdout resampling
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
```

There are 506 records with one continuous response `medv` (median house value) and 13 predictors. We again create a training set.

### Best Sub-Tree Model

```{r}
cv.boston=cv.tree(object=tree.boston,
                  K=10) # k-fold cross-validation
cv.boston
```

We use the `cv.tree()` function to perform 10-fold cross validation to find the best subtree. _We do not need to specify the argument FUN since it is the regression tree._

```{r}
prune.boston=prune.tree(tree.boston,
                        best=8) # number of terminal nodes
plot(prune.boston)
text(prune.boston,pretty=0)
```

We prune tree to the optimal size using the `prune.tree()` function and plot the tree.

### Tree Model Performance

```{r}
# assign vector of the target variable medv as test data
boston.test=Boston[-train,"medv"] 

tree.pred=predict(prune.boston, # pruned tree model
                  newdata=Boston[-train,]) # test data

# calculate MSE
mean((tree.pred-boston.test)^2) 
```

Then we use the pruned tree to make predictions in the testing set and calculate the mean square error (MSE).

## Random Forests (FULL) Model

```{r}
library(randomForest)

set.seed(1)

bag.boston=randomForest(medv~., # target/response variable
                        data=Boston, # df containing the variable of the model
                        subset=train, # train dataset to model
                        mtry=13, # number of predictors to use (all 13)
                        importance=TRUE)
bag.boston
```

In the `randomForest()` function, the argument `mtry=13` indicates that *all 13 predictors* should be used, that is bagging. `subset=train` indicates that we train this model only using the training dataset. `importance=TRUE` indicates that the importance of predictors is assessed. `bag.boston` stores the bagging model.


### Random Forest (FULL) Model Performance

```{r}
yhat.bag = predict(bag.boston, # rf model
                   newdata=Boston[-train,]) # test data

# Calculate MSE mean sq. of the difference predicted and actual
mean((yhat.bag-boston.test)^2) 
```

We next evaluate the performance of bagging by fitting it to the testing dataset Boston[-train,]. Then we calculate the MSE.

### Random Forest Model

```{r}
set.seed(1)

# target response medv against any 5 predictors (mtry=5)
rf.boston=randomForest(medv~.,
                       data=Boston,
                       subset=train,
                       mtry=5, # rf regression uses 1/3 of total predictors
                       importance=TRUE)

yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

Now letâ€™s use the function to implement random forest. The *difference is that random forest does not use all input variables in each tree*. It usually uses **p/3 variables for regression trees**  and **$\sqrt{p}$ for classification trees**. Now letâ€™s use `mtry=5`. `rf.boston` stores the random forest model. We test the MSE of this model by comparing the predicted values with the true values.


```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```

The importance of each variable can be evaluated using the `importance()` function. The function `varImpPlot()` plots the important measures. Two measures of variable importance are reported. One is based on the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees.


# Clustering

## Load Data

```{r}
iris<-read.csv("../raw_data/iris.csv",
               header=T,
               stringsAsFactors=TRUE)
names(iris)
attach(iris)
dim(iris)
summary(iris)
```

`iris.csv` is a multivariate data set introduced by Ronald Fisher (1936): The use of multiple measurements in taxonomic problem. It contains 3 classes of a total 150 instances. Each class refers to a type of iris plant. There are five variables in this data set: `Sepal.Length, Sepal.Width, Petal.Length, Petal.Width and Species`.

```{r}
#| output: true
iris.labs=iris[,5]
iris.data=iris[,1:4]
dim(iris.data)
```

Each instance is labeled with a class. We do not use the class in performing clustering, as it is an unsupervised technique. But after performing clustering, we can check the extent to which these classes agree with the result of the unsupervised technique. Accordingly, we store the label of each instance in the object `iris.labs`. 

```{r}
#| output: true

table(iris.labs)
```

`table(iris.labs)` shows that we have 50 instances in each class.


### Euclidean Distance Method

```{r}
data.dist=dist(iris.data,
               method = 'euclidean')
```


Since clustering relies on the distances between clusters, we use the function `dist()` to compute the $150 \times 150$ inter-observation Euclidean distance matrix and store it in the object `data.dist`.

#### Hierarchical Clustering 

```{r}
# default uses complete linkage
hc1=hclust(d=data.dist)

# average linkage 
hc2=hclust(d=data.dist, 
           method="average")

# single linkage
hc3=hclust(d=data.dist, 
           method="single")
```

The `hclust()` function implements hierarchical clustering. The first parameter in this function is the dissimilarity structure `d=data.dist`, i.e. the distance matrix. The second parameter `method=` is the linkage type, e.g. average, complete, single, or centroid. By default, it is the â€œcompleteâ€ linkage type. Thus, hc1 stores the clustering dendrogram using the complete linkage clustering.

```{r}
# combine plots in 1x3 display 
par(mfrow=c(1,3)) 

plot(hc1, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hc2, main="Average Linkage", xlab="", sub="",ylab="")
plot(hc3, main="Single Linkage", xlab="", sub="",ylab="")
```

We plot the obtained dendrograms using the `plot()` function. The numbers at the bottom of the plot identify each observation. We could see that the choice of linkage certainly affect the results obtained.

```{r}
hc.clusters1=cutree(hc1,3)
hc.clusters2=cutree(hc2,3)
hc.clusters3=cutree(hc3,3)
```

We need to determine where to cut the dendrogram so as to identify the labels for each observation. This decision has a strong impact on the clustering results. We usually need to try several choices, and use the one with the highest interpretability. In this case, as we already know there are a total of three classes, we can cut them to obtain three clusters using the `cutree()` function.

```{r}
#| output: true
table(hc.clusters1,iris.labs)
table(hc.clusters2,iris.labs)
table(hc.clusters3,iris.labs)
```

We then compare the cluster labels from all three methods with the original labels. All three linkage methods successfully identify all the flowers of species setosa into cluster 1. But they do not differentiate between virginica and versicolor quite well. The average linkage type looks better than the other two types.

```{r}
#| output: true

# display 2 plots in 1x2 format
par(mfrow=c(1,2))

# set color configuration
cols <- c("red","green","blue")

plot(Sepal.Width ~ Sepal.Length, 
     data=iris, 
     col=cols[iris$Species], 
     main="Sepal.Width ~ Sepal.Length")
legend(x=6.5, y=4.5, 
       legend=levels(iris$Species), 
       col=cols, 
       pch=1)

plot(Petal.Width ~ Petal.Length, 
     data=iris, 
     col=cols[iris$Species],
     main="Petal.Width ~ Petal.Length")
legend(x=1, y=2.5, 
       legend=levels(iris$Species), 
       col=cols, 
       pch=1)
```

Based on the above two plots, variables Petal.Length and Petal.Width can better differentiate among the three species. Thus, we re-design all three clustering models using the only two variables.

### Hierarchical Clustering Model Comparison on Selected Variables

```{r}
# include only Petal.Length and Petal.Width
iris.data2=iris[,3:4]

# Euclidean distance 
data.dist2=dist(iris.data2, 
                method = 'euclidean')

# cluster model default is method=complete
newhc1=hclust(data.dist2)

# average linkage
newhc2=hclust(data.dist2, method="average")

# single linkage
newhc3=hclust(data.dist2, method="single")
```

Re-design all three clustering models using the only two variables.

```{r}
# make it into 3 clusters (groups)
newhc.clusters1=cutree(newhc1,3)
newhc.clusters2=cutree(newhc2,3)
newhc.clusters3=cutree(newhc3,3)
```

Make into 3 cluster groups on all three methods

```{r}
#| output: true

# analyze difference between difference hierarchical methods
# complete linkage
table(newhc.clusters1,iris.labs) 
# average linkage
table(newhc.clusters2,iris.labs)
# single linkage
table(newhc.clusters3,iris.labs)
```

Now we can see that the clustering algorithm with the average linkage does a much better job. Only five observations in versicolor and one in virginica do not fall in their own cluster.

### Plot the best method, Average Linkage

```{r}
#| output: true
par(mfrow=c(1,1))
plot(newhc2, labels=iris.labs)
abline(h=1.3, col="red")
newhc2
```

## K-Means Clustering

```{r}
set.seed(1)
km.out1 =kmeans(iris.data,
                centers=3, # number of k-clusters
                nstart=1) # how many random sets
```

In Step 1 of K-means, the initial cluster labels are assigned randomly, we first set a random seed. The function `kmeans()` performs K-means clustering. The first parameter is the data matrix. The second one is the pre-specified `K`, the number of clusters. In this case, we set it as 3. The third parameter, `nstart`, indicates the number of random assignments in Step 1. We first set `nstart=1`, which means that only run the function with one set of initial cluster assignment.

```{r}
#| output: true
km.out1
km.out1$cluster
km.out1$betweenss
km.out1$withinss
km.out1$tot.withinss
km.out1$totss
```

The kmeans function returns multiple values. `km.out1$cluster` returns the clustering results. `km.out1$betweenss` returns the between-cluster sum of squares. `km.out1$withinss` returns the within-cluster sum of squares for each cluster.  `km.out1$tot.withinss` returns the total within-cluster sum of squares across all K clusters. `km.out1$totss`returns the total sum of squares in this data set.

```{r}
table(km.out1$cluster,iris.labs)
```

We compare the cluster labels with the original labels by using the function `table()`.

```{r}
#| output: true
set.seed(3)
km.out2=kmeans(iris.data,
               centers=3, # number of k-clusters
               nstart=1) # how many random sets

km.out2$betweenss
km.out2$withinss
km.out2$tot.withinss
km.out2$totss
table(km.out2$cluster,iris.labs)
```

Now letâ€™s explore if the initial cluster assignment affects the clustering results. We set a different random seed for Step 1 and then re-run the algorithm. Do we get the same clustering results? Doe the total within cluster sum of squares change? Does the total sum of squares change?

```{r}
set.seed(1)
km.out3 =kmeans(iris.data, 
                centers=3, # number of k-cluster
                nstart=20) # how many random sets
km.out3$betweenss
km.out3$withinss
km.out3$tot.withinss
km.out3$totss
table(km.out3$cluster,iris.labs)
```

We use nstart=1in the previous two models. If a value of nstart greater than one is used, then K-means clustering will be performed using multiple random assignments. It reports only the best results in terms of the total within-cluster
sum of squares. Now letâ€™s try `nstart=20`. In practice, we usually try a larger value of `nstart`, e.g. 20, 50, to obtain the optimal model. Do we get the same clustering results as the previous two models? Doe the total within-cluster sum of squares change? Does the total sum of squares change?

```{r}
iris.data2=iris[,3:4]
```

As we learnt in hierarchical clustering, variables Petal.Length and Petal.Width can better differentiate among the three species. Thus, we run models using the only two variables.

```{r}
#| output: true

set.seed(1)

# scale data
sd.data=scale(iris.data2)

km.out4=kmeans(sd.data,
                centers=3, # number of k-clusters
                nstart =20) # how many random sets

km.out4$betweenss
km.out4$withinss
km.out4$tot.withinss
km.out4$totss

table(km.out4$cluster,iris.labs)
```

If we intend to standardize the variables to have mean zero and standard deviation one, we can use the `scale()` function. Again, do we get the same clustering results as the previous two models? Doe the total within-cluster sum of squares change? Does the total sum of squares change? Why?


```{r}
km.out5=kmeans(iris.data2,
               centers=3, # number of k-clusters 
               nstart=20) # how many random
km.out5$betweenss
km.out5$withinss
km.out5$tot.withinss
km.out5$totss
table(km.out5$cluster,iris.labs)
```

We can compare the results with the model without variable standardization.

```{r}
wss=km.out5$totss

for (i in 2:10)
{
  wss[i] = sum(kmeans(iris.data2,centers=i)$withinss)
} 


plot(1:10, wss, 
     type="b", 
     xlab="Number of Clusters", 
     ylab="Within groups sum of squares", 
     main="find the optimal value of K")
```


Once again, do we get the clustering results as the previous two models? Doe the total within-cluster sum of squares change? Does the total sum of squares change?
We use `K=3` in this case because of our prior knowledge. Now letâ€™s use `km.out5` to check if `K=3` is an optimal choice (you can also check it using other models).  We calculate the total within-cluster sum of squares from `K=1` to `K=10`. Based on the plot, `K=3` is a good choice.